{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://docs.google.com/uc?export=download&id=1Juwx8HtDwSzmVIJ31ooVa1WljI4U5JnA&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1Zy6BZH_zLEjKlEFSduKE5tV9qqA_8VtM&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1VUcBGr0T0-klqerjAjxRmAqFuld_SMWU&confirm=t\n",
      "Downloading https://docs.google.com/uc?export=download&id=1NI5pa5Chpd-52eSmLW60OnB3WS5ikxq_&confirm=t\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attack_method='GTA', cuda=True, dataset='Yelp', debug=True, defense_mode='none', device_id=0, dis_weight=1, dropout=0.5, epochs=200, hidden=32, homo_boost_thrd=0.5, homo_loss_weight=0, inner=1, lr=0.01, model='GCN', no_cuda=False, prune_thr=0.3, seed=10, selection_method='none', target_class=0, target_loss_weight=1, test_model='GCN', thrd=0.5, trigger_prob=0.5, trigger_size=3, trojan_epochs=200, vs_ratio=0.01, weight_decay=0.0005)\n",
      "Length of training set: 143369\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "0D or 1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 137>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=144'>145</a>\u001b[0m t_total \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=145'>146</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLength of training set: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mlen\u001b[39m(idx_train)))\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=146'>147</a>\u001b[0m benign_model\u001b[39m.\u001b[39;49mfit(data\u001b[39m.\u001b[39;49mx, train_edge_index, \u001b[39mNone\u001b[39;49;00m, data\u001b[39m.\u001b[39;49my, idx_train, idx_val,train_iters\u001b[39m=\u001b[39;49margs\u001b[39m.\u001b[39;49mepochs,verbose\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTraining benign model Finished!\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#W0sdnNjb2RlLXJlbW90ZQ%3D%3D?line=148'>149</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal time elapsed: \u001b[39m\u001b[39m{:.4f}\u001b[39;00m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m t_total))\n",
      "File \u001b[0;32m/home/project-graph-backdoor/Backdoor/models/GCN.py:79\u001b[0m, in \u001b[0;36mGCN.fit\u001b[0;34m(self, features, edge_index, edge_weight, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_without_val(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels, idx_train, train_iters, verbose)\n\u001b[1;32m     78\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_with_val(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabels, idx_train, idx_val, train_iters, verbose)\n\u001b[1;32m     80\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "File \u001b[0;32m/home/project-graph-backdoor/Backdoor/models/GCN.py:111\u001b[0m, in \u001b[0;36mGCN._train_with_val\u001b[0;34m(self, labels, idx_train, idx_val, train_iters, verbose)\u001b[0m\n\u001b[1;32m    109\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    110\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_index, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_weight)\n\u001b[0;32m--> 111\u001b[0m loss_train \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mnll_loss(output[idx_train], labels[idx_train])\n\u001b[1;32m    112\u001b[0m loss_train\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    113\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/py38_torch110/lib/python3.8/site-packages/torch/nn/functional.py:2689\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2687\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2688\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2689\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: 0D or 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "from models.GCN import GCN\n",
    "from models.GCN_Encoder import GCN_Encoder\n",
    "from torch_geometric.datasets import Planetoid, WebKB, WikipediaNetwork,Reddit,Flickr,Yelp\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "# from torch_geometric.loader import DataLoader\n",
    "from help_funcs import prune_unrelated_edge,prune_unrelated_edge_isolated,select_target_nodes\n",
    "import help_funcs\n",
    "import scipy.sparse as sp\n",
    "\n",
    "# Training settings\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--debug', action='store_true',\n",
    "        default=True, help='debug mode')\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--seed', type=int, default=10, help='Random seed.')\n",
    "parser.add_argument('--model', type=str, default='GCN', help='model',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'])\n",
    "parser.add_argument('--dataset', type=str, default='Yelp', help='Dataset',\n",
    "                    choices=['Cora','Citeseer','Pubmed','PPI','Flickr','ogbn-arxiv','Reddit','Reddit2','Yelp'])\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=32,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--thrd', type=float, default=0.5)\n",
    "parser.add_argument('--target_class', type=int, default=0)\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--epochs', type=int,  default=200, help='Number of epochs to train benign and backdoor model.')\n",
    "parser.add_argument('--trojan_epochs', type=int,  default=200, help='Number of epochs to train trigger generator.')\n",
    "parser.add_argument('--inner', type=int,  default=1, help='Number of inner')\n",
    "# backdoor setting\n",
    "parser.add_argument('--trigger_size', type=int, default=3,\n",
    "                    help='tirgger_size')\n",
    "parser.add_argument('--vs_ratio', type=float, default=0.01,\n",
    "                    help=\"ratio of poisoning nodes relative to the full graph\")\n",
    "# defense setting\n",
    "parser.add_argument('--defense_mode', type=str, default=\"none\",\n",
    "                    choices=['prune', 'isolate', 'none'],\n",
    "                    help=\"Mode of defense\")\n",
    "parser.add_argument('--prune_thr', type=float, default=0.3,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "parser.add_argument('--target_loss_weight', type=float, default=1,\n",
    "                    help=\"Weight of optimize outter trigger generator\")\n",
    "parser.add_argument('--homo_loss_weight', type=float, default=0,\n",
    "                    help=\"Weight of optimize similarity loss\")\n",
    "parser.add_argument('--homo_boost_thrd', type=float, default=0.5,\n",
    "                    help=\"Threshold of increase similarity\")\n",
    "# attack setting\n",
    "parser.add_argument('--dis_weight', type=float, default=1,\n",
    "                    help=\"Weight of cluster distance\")\n",
    "parser.add_argument('--attack_method', type=str, default='GTA',\n",
    "                    choices=['Rand_Gene','Rand_Samp','GTA','None'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--trigger_prob', type=float, default=0.5,\n",
    "                    help=\"The probability to generate the trigger's edges in random method\")\n",
    "parser.add_argument('--selection_method', type=str, default='none',\n",
    "                    choices=['loss','conf','cluster','none'],\n",
    "                    help='Method to select idx_attach for training trojan model (none means randomly select)')\n",
    "parser.add_argument('--test_model', type=str, default='GCN',\n",
    "                    choices=['GCN','GAT','GraphSage','GIN'],\n",
    "                    help='Model used to attack')\n",
    "\n",
    "# GPU setting\n",
    "parser.add_argument('--device_id', type=int, default=0,\n",
    "                    help=\"Threshold of prunning edges\")\n",
    "# args = parser.parse_args()\n",
    "args = parser.parse_known_args()[0]\n",
    "args.cuda =  not args.no_cuda and torch.cuda.is_available()\n",
    "device = torch.device(('cuda:{}' if torch.cuda.is_available() else 'cpu').format(args.device_id))\n",
    "\n",
    "\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "np.random.seed(11) # fix the random seed is important\n",
    "if(args.dataset == 'Cora' or args.dataset == 'Citeseer' or args.dataset == 'Pubmed'):\n",
    "    dataset = Planetoid(root='./data/', \\\n",
    "                        name=args.dataset,\\\n",
    "                        transform=transform)\n",
    "elif(args.dataset == 'Flickr'):\n",
    "    dataset = Flickr(root='./data/Flickr/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit'):\n",
    "    dataset = Reddit(root='./data/Reddit/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'Reddit2'):\n",
    "    dataset = Reddit(root='./data/Reddit2/', \\\n",
    "                    transform=transform)\n",
    "elif(args.dataset == 'ogbn-arxiv'):\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./data/')\n",
    "    split_idx = dataset.get_idx_split() \n",
    "elif(args.dataset == 'Yelp'):\n",
    "    # Download and process data at './dataset/ogbg_molhiv/'\n",
    "    dataset = Yelp(root='./data/')\n",
    "    # idx_train, idx_val, idx_test = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "\n",
    "data = dataset[0].to(device)\n",
    "# we build our own train test split \n",
    "from utils import get_split\n",
    "data, idx_train, idx_val, idx_clean_test, idx_atk = get_split(data,device)\n",
    "\n",
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "from utils import subgraph\n",
    "data.edge_index = to_undirected(data.edge_index)\n",
    "train_edge_index,_, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "\n",
    "# In[3]:\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.cuda.manual_seed(args.seed)\n",
    "print(args)\n",
    "\n",
    "# In[6]: \n",
    "import os\n",
    "from models.backdoor import model_construct\n",
    "benign_modelpath = './modelpath/{}_{}_benign.pth'.format(args.model, args.dataset)\n",
    "if(os.path.exists(benign_modelpath)):\n",
    "    # load existing benign model\n",
    "    benign_model = torch.load(benign_modelpath)\n",
    "    benign_model = benign_model.to(device)\n",
    "    # edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "else:\n",
    "    benign_model = model_construct(args,args.model,data,device).to(device) \n",
    "    t_total = time.time()\n",
    "    print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "    benign_model.fit(data.x, train_edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=False)\n",
    "    print(\"Training benign model Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    # Save trained model\n",
    "    torch.save(benign_model, benign_modelpath)\n",
    "    print(\"Benign model saved at {}\".format(benign_modelpath))\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, None, data.y,idx_clean_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "# In[9]:\n",
    "\n",
    "from sklearn_extra import cluster\n",
    "from models.backdoor import obtain_attach_nodes,Backdoor,obtain_attach_nodes_by_influential,obtain_attach_nodes_by_cluster\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = obtain_attach_nodes(unlabeled_idx,size)\n",
    "elif(args.selection_method == 'loss' or args.selection_method == 'conf'):\n",
    "    idx_attach = obtain_attach_nodes_by_influential(args,benign_model,unlabeled_idx.cpu().tolist(),data.x,train_edge_index,None,data.y,device,size,selected_way=args.selection_method)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    # construct GCN encoder\n",
    "    encoder_modelpath = './modelpath/{}_{}_benign.pth'.format('GCN_Encoder', args.dataset)\n",
    "    if(os.path.exists(encoder_modelpath)):\n",
    "        # load existing benign model\n",
    "        gcn_encoder = torch.load(encoder_modelpath)\n",
    "        gcn_encoder = gcn_encoder.to(device)\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Loading {} encoder Finished!\".format(args.model))\n",
    "    else:\n",
    "        gcn_encoder = model_construct(args,'GCN_Encoder',data,device).to(device) \n",
    "        t_total = time.time()\n",
    "        # edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "        gcn_encoder.fit(data.x, train_edge_index, None, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "        print(\"Training encoder Finished!\")\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        # # Save trained model\n",
    "        # torch.save(gcn_encoder, encoder_modelpath)\n",
    "        # print(\"Encoder saved at {}\".format(encoder_modelpath))\n",
    "    # test gcn encoder \n",
    "    encoder_clean_test_ca = gcn_encoder.test(data.x, data.edge_index, None, data.y,idx_clean_test)\n",
    "    print(\"Encoder CA on clean test nodes: {:.4f}\".format(encoder_clean_test_ca))\n",
    "    # from sklearn import cluster\n",
    "    seen_node_idx = torch.concat([idx_train,unlabeled_idx])\n",
    "    nclass = np.unique(data.y.cpu().numpy()).shape[0]\n",
    "    encoder_x = gcn_encoder.get_h(data.x, train_edge_index,None).clone().detach()\n",
    "    kmedoids = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    kmedoids.fit(encoder_x[seen_node_idx].detach().cpu().numpy())\n",
    "    idx_attach = obtain_attach_nodes_by_cluster(args,kmedoids,unlabeled_idx.cpu().tolist(),encoder_x,data.y,device,size)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "\n",
    "# In[10]:\n",
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "if(args.attack_method == 'GTA'):\n",
    "    model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned()\n",
    "elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "    model.fit_rand(data.x, train_edge_index, None, data.y, idx_train,idx_attach, unlabeled_idx)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = model.get_poisoned_rand()\n",
    "elif(args.attack_method == 'None'):\n",
    "    train_edge_weights = torch.ones([train_edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    poison_x, poison_edge_index, poison_edge_weights, poison_labels = data.x.clone(), train_edge_index.clone(), train_edge_weights, data.y.clone()\n",
    "\n",
    "# In[12]:\n",
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "\n",
    "print(\"precent of left attach nodes: {:.3f}\"\\\n",
    "    .format(len(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))/len(idx_attach)))\n",
    "#%%\n",
    "test_model = model_construct(args,args.test_model,data,device).to(device) \n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=200,verbose=False)\n",
    "\n",
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "\n",
    "# %% inject trigger on attack test nodes (idx_atk)'''\n",
    "if(args.attack_method == 'GTA'):\n",
    "    induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,poison_x,induct_edge_index,induct_edge_weights)\n",
    "elif(args.attack_method == 'Rand_Gene' or args.attack_method == 'Rand_Samp'):\n",
    "    induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger_rand(idx_atk,poison_x,induct_edge_index,induct_edge_weights,data.y)\n",
    "elif(args.attack_method == 'None'):\n",
    "    induct_x, induct_edge_index,induct_edge_weights = poison_x,induct_edge_index,induct_edge_weights\n",
    "# do pruning in test datas'''\n",
    "if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "    induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "# attack evaluation\n",
    "\n",
    "output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_clean_test)\n",
    "print(\"CA: {:.4f}\".format(ca))\n",
    "\n",
    "# %%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Cora()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# splitted_idx = data.get_idx_split()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m idx_train, idx_val, idx_test \u001b[39m=\u001b[39m split_idx[\u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m], split_idx[\u001b[39m\"\u001b[39m\u001b[39mvalid\u001b[39m\u001b[39m\"\u001b[39m], split_idx[\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bszw494-root/home/project-graph-backdoor/Backdoor/run_adaptive.ipynb#X41sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m graph, labels \u001b[39m=\u001b[39m dataset[\u001b[39m0\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# from ogb.graphproppred import PygGraphPropPredDataset\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# Download and process data at './dataset/ogbg_molhiv/'\n",
    "dataset = PygNodePropPredDataset(name = 'ogbn-arxiv', root='./dataset/')\n",
    "\n",
    "split_idx = dataset.get_idx_split() \n",
    "# splitted_idx = data.get_idx_split()\n",
    "idx_train, idx_val, idx_test = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
    "graph, labels = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(num_nodes=169343, edge_index=[2, 1166243], x=[169343, 128], node_year=[169343, 1], y=[169343, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]\n",
    "# train_loader = DataLoader(dataset[split_idx['train']])\n",
    "# valid_loader = DataLoader(dataset[split_idx['valid']])\n",
    "# test_loader = DataLoader(dataset[split_idx['test']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.x\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.tx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.allx\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.y\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ty\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.ally\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.graph\n",
      "Downloading https://github.com/kimiyoung/planetoid/raw/master/data/ind.cora.test.index\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "import torch_geometric.transforms as T\n",
    "transform = T.Compose([T.NormalizeFeatures()])\n",
    "\n",
    "# if args.dataset in ['Cora', 'Citeseer', 'Pubmed']:\n",
    "dataset = Planetoid(root='./data/', split=\"random\", num_train_per_class=80, num_val=400, num_test=1000, \\\n",
    "                    name=args.dataset,transform=None)\n",
    "# dataset = Reddit(root='./data/', transform=transform, pre_transform=None)\n",
    "# dataset = classFlickr(root='./data/', transform=transform, pre_transform=None)\n",
    "\n",
    "data = dataset[0].to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "from torch_geometric.utils import to_undirected\n",
    "# get the overall edge index of the graph\n",
    "data.edge_index = to_undirected(data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%  mask the test nodes\n",
    "from utils import subgraph\n",
    "# get the edge index used for training (except from test nodes) and \n",
    "train_edge_index,train_edge_weights, edge_mask = subgraph(torch.bitwise_not(data.test_mask),data.edge_index,relabel_nodes=False)\n",
    "\n",
    "mask_edge_index = data.edge_index[:,torch.bitwise_not(edge_mask)]\n",
    "idx_train =data.train_mask.nonzero().flatten()\n",
    "idx_val = data.val_mask.nonzero().flatten()\n",
    "idx_test = data.test_mask.nonzero().flatten()\n",
    "# val_mask = node_idx[data.val_mask]\n",
    "# labels = data.y[torch.bitwise_not(data.test_mask)]\n",
    "# features = data.x[torch.bitwise_not(data.test_mask)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0028799998253884154\n"
     ]
    }
   ],
   "source": [
    "e = data.edge_index.shape[1]\n",
    "t = data.x.shape[0]\n",
    "p = 2*e/(t*(t-1))\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "from models.SAGE import GraphSage\n",
    "def model_construct(args,model_name,data):\n",
    "    if (model_name == 'GCN'):\n",
    "        model = GCN(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    elif(model_name == 'GAT'):\n",
    "        model = GAT(nfeat=data.x.shape[1], \n",
    "                    nhid=args.hidden, \n",
    "                    nclass=int(data.y.max()+1), \n",
    "                    heads=8,\n",
    "                    dropout=args.dropout, \n",
    "                    lr=args.lr, \n",
    "                    weight_decay=args.weight_decay, \n",
    "                    device=device)\n",
    "    elif(model_name == 'GraphSage'):\n",
    "        model = GraphSage(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    elif(model_name == 'GCN_Encoder'):\n",
    "        model = GCN_Encoder(nfeat=data.x.shape[1],\n",
    "                    nhid=args.hidden,\n",
    "                    nclass= int(data.y.max()+1),\n",
    "                    dropout=args.dropout,\n",
    "                    lr=args.lr,\n",
    "                    weight_decay=args.weight_decay,\n",
    "                    device=device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "train benign model\n",
    "'''\n",
    "import os\n",
    "benign_modelpath = './modelpath/{}_{}_benign.pth'.format(args.model, args.dataset)\n",
    "if(os.path.exists(benign_modelpath) and args.load_benign_model):\n",
    "    # load existing benign model\n",
    "    benign_model = torch.load(benign_modelpath)\n",
    "    benign_model = benign_model.to(device)\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "else:\n",
    "    benign_model = model_construct(args,args.model,data).to(device) \n",
    "    t_total = time.time()\n",
    "    edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "    print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "    benign_model.fit(data.x, train_edge_index, train_edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "    print(\"Training benign model Finished!\")\n",
    "    print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "    # Save trained model\n",
    "    torch.save(benign_model, benign_modelpath)\n",
    "    print(\"Benign model saved at {}\".format(benign_modelpath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benign CA: 0.8410\n",
      "Benign CA on clean test nodes: 0.8250\n"
     ]
    }
   ],
   "source": [
    "benign_output = benign_model(data.x, data.edge_index, edge_weights)\n",
    "benign_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "print(\"Benign CA: {:.4f}\".format(benign_ca))\n",
    "atk_test_nodes, clean_test_nodes,poi_train_nodes = select_target_nodes(args,args.seed,benign_model,data.x, data.edge_index, edge_weights,data.y,idx_val,idx_test)\n",
    "clean_test_ca = benign_model.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "print(\"Benign CA on clean test nodes: {:.4f}\".format(clean_test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading benign GCN model Finished!\n",
      "Encoder CA: 0.8380\n",
      "Encoder CA on clean test nodes: 0.8000\n",
      "[4 4 3 ... 4 4 4]\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from sklearn_extra import cluster\n",
    "from models.backdoor import obtain_attach_nodes,Backdoor,obtain_attach_nodes_by_influential,obtain_attach_nodes_by_cluster\n",
    "# filter out the unlabeled nodes except from training nodes and testing nodes, nonzero() is to get index, flatten is to get 1-d tensor\n",
    "unlabeled_idx = (torch.bitwise_not(data.test_mask)&torch.bitwise_not(data.train_mask)).nonzero().flatten()\n",
    "# poison nodes' size\n",
    "size = int((len(data.test_mask)-data.test_mask.sum())*args.vs_ratio)\n",
    "# here is randomly select poison nodes from unlabeled nodes\n",
    "if(args.selection_method == 'none'):\n",
    "    idx_attach = obtain_attach_nodes(unlabeled_idx,size)\n",
    "elif(args.selection_method == 'loss' or args.selection_method == 'conf'):\n",
    "    idx_attach = obtain_attach_nodes_by_influential(args,benign_model,unlabeled_idx.cpu().tolist(),data.x,train_edge_index,train_edge_weights,data.y,device,size,selected_way=args.selection_method)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "elif(args.selection_method == 'cluster'):\n",
    "    # construct GCN encoder\n",
    "    encoder_modelpath = './modelpath/{}_{}_benign.pth'.format('GCN_Encoder', args.dataset)\n",
    "    if(os.path.exists(encoder_modelpath) and args.load_benign_model):\n",
    "        # load existing benign model\n",
    "        gcn_encoder = torch.load(encoder_modelpath)\n",
    "        gcn_encoder = gcn_encoder.to(device)\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Loading benign {} model Finished!\".format(args.model))\n",
    "    else:\n",
    "        gcn_encoder = model_construct(args,'GCN_Encoder',data).to(device) \n",
    "        t_total = time.time()\n",
    "        edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "        print(\"Length of training set: {}\".format(len(idx_train)))\n",
    "        gcn_encoder.fit(data.x, train_edge_index, train_edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "        print(\"Training encoder Finished!\")\n",
    "        print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "        # Save trained model\n",
    "        torch.save(gcn_encoder, encoder_modelpath)\n",
    "        print(\"Encoder saved at {}\".format(encoder_modelpath))\n",
    "    # test gcn encoder \n",
    "    encoder_benign_ca = gcn_encoder.test(data.x, data.edge_index, edge_weights, data.y,idx_test)\n",
    "    print(\"Encoder CA: {:.4f}\".format(encoder_benign_ca))\n",
    "    encoder_clean_test_ca = gcn_encoder.test(data.x, data.edge_index, edge_weights, data.y,clean_test_nodes)\n",
    "    print(\"Encoder CA on clean test nodes: {:.4f}\".format(encoder_clean_test_ca))\n",
    "    # from sklearn import cluster\n",
    "    seen_node_idx = torch.concat([idx_train,unlabeled_idx])\n",
    "    nclass = np.unique(data.y.cpu().numpy()).shape[0]\n",
    "    # kmeans = cluster.KMeans(n_clusters=nclass,random_state=1)\n",
    "    # kmeans.fit(data.x[seen_node_idx].cpu().numpy())\n",
    "    # # unlabeled_idx.cpu().tolist()\n",
    "\n",
    "    # train_adj = to_dense_adj(train_edge_index,edge_attr=train_edge_weights)[0].cpu()\n",
    "    # train_adj = train_adj + train_adj @ train_adj\n",
    "    # train_adj = torch.where(train_adj>0, torch.tensor(1.0),\n",
    "    #                                             torch.tensor(0.0))\n",
    "    # train_x = train_adj @ data.x.cpu()\n",
    "    # new_train_edge_index, new_train_edge_weights= dense_to_sparse(train_adj)\n",
    "    # kmeans = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    # # kmeans.fit(data.x[seen_node_idx].detach().cpu().numpy())\n",
    "    # kmeans.fit(train_x.detach().cpu().numpy())\n",
    "    # idx_attach = obtain_attach_nodes_by_cluster(args,kmeans,unlabeled_idx.cpu().tolist(),train_x,data.y,device,size)\n",
    "    # idx_attach = torch.LongTensor(idx_attach).to(device)\n",
    "    \n",
    "    encoder_x = gcn_encoder.get_h(data.x, train_edge_index,train_edge_weights).clone().detach()\n",
    "    kmeans = cluster.KMedoids(n_clusters=nclass,method='pam')\n",
    "    # kmeans.fit(data.x[seen_node_idx].detach().cpu().numpy())\n",
    "    kmeans.fit(encoder_x.detach().cpu().numpy())\n",
    "    idx_attach = obtain_attach_nodes_by_cluster(args,kmeans,unlabeled_idx.cpu().tolist(),encoder_x,data.y,device,size)\n",
    "    idx_attach = torch.LongTensor(idx_attach).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "tensor(1.9393, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(20.4000, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 0, training loss: 1.9392701387405396\n",
      "acc_train_clean: 0.1768, acc_train_attach: 0.0588\n",
      "tensor(1.7866, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(9.8495, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.6262, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(13.2455, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.4436, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(11.8224, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.3043, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(10.5700, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.1444, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(9.4848, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(1.0180, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(8.4280, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.9117, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(6.9387, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.8150, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(5.7199, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.7289, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(4.5158, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.6040, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(3.5913, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "Epoch 10, training loss: 0.6040130853652954\n",
      "acc_train_clean: 0.9000, acc_train_attach: 0.7059\n",
      "tensor(0.5469, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.7172, device='cuda:2', grad_fn=<MulBackward0>)\n",
      "tensor(0.5104, device='cuda:2', grad_fn=<NllLossBackward0>) tensor(2.5027, device='cuda:2', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# train trigger generator \n",
    "model = Backdoor(args,device)\n",
    "print(args.epochs)\n",
    "model.fit(data.x, train_edge_index, None, data.y, idx_train,idx_attach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "poison_x = model.poison_x.data\n",
    "poison_edge_index = model.poison_edge_index.data\n",
    "poison_edge_weights = model.poison_edge_weights.data\n",
    "poison_labels = model.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(args.defense_mode == 'prune'):\n",
    "    poison_edge_index,poison_edge_weights = prune_unrelated_edge(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)\n",
    "elif(args.defense_mode == 'isolate'):\n",
    "    poison_edge_index,poison_edge_weights,rel_nodes = prune_unrelated_edge_isolated(args,poison_edge_index,poison_edge_weights,poison_x,device)\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).tolist()\n",
    "    bkd_tn_nodes = torch.LongTensor(list(set(bkd_tn_nodes) - set(rel_nodes))).to(device)\n",
    "else:\n",
    "    bkd_tn_nodes = torch.cat([idx_train,idx_attach]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "577\n",
      "577\n",
      "4306 2880\n",
      "{1569, 2081, 1699, 1251, 133, 2185, 843, 1775, 303, 305, 1780, 1814, 1782, 2143, 1243, 2046, 1791}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(torch.cat([idx_train,idx_attach])))\n",
    "print(len(bkd_tn_nodes))\n",
    "print(len(model.poison_edge_index.data[0]),len(poison_edge_index[0]))\n",
    "# print(idx_attach & bkd_tn_nodes)\n",
    "print(set(bkd_tn_nodes.tolist()) & set(idx_attach.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== training gcn model ===\n",
      "Epoch 0, training loss: 1.9486966133117676\n",
      "acc_val: 0.3725\n",
      "Epoch 10, training loss: 0.5837284326553345\n",
      "acc_val: 0.7350\n",
      "Epoch 20, training loss: 0.22602947056293488\n",
      "acc_val: 0.7625\n",
      "Epoch 30, training loss: 0.126153364777565\n",
      "acc_val: 0.7625\n",
      "Epoch 40, training loss: 0.09417053312063217\n",
      "acc_val: 0.7625\n",
      "Epoch 50, training loss: 0.08149732649326324\n",
      "acc_val: 0.7525\n",
      "Epoch 60, training loss: 0.08437832444906235\n",
      "acc_val: 0.7425\n",
      "Epoch 70, training loss: 0.07754302769899368\n",
      "acc_val: 0.7500\n",
      "Epoch 80, training loss: 0.08388686925172806\n",
      "acc_val: 0.7575\n",
      "Epoch 90, training loss: 0.0656595528125763\n",
      "acc_val: 0.7525\n",
      "Epoch 100, training loss: 0.0655829906463623\n",
      "acc_val: 0.7475\n",
      "Epoch 110, training loss: 0.06835032254457474\n",
      "acc_val: 0.7600\n",
      "Epoch 120, training loss: 0.060565438121557236\n",
      "acc_val: 0.7525\n",
      "Epoch 130, training loss: 0.07300901412963867\n",
      "acc_val: 0.7550\n",
      "Epoch 140, training loss: 0.05255601555109024\n",
      "acc_val: 0.7500\n",
      "Epoch 150, training loss: 0.05699390545487404\n",
      "acc_val: 0.7500\n",
      "Epoch 160, training loss: 0.0540340282022953\n",
      "acc_val: 0.7450\n",
      "Epoch 170, training loss: 0.060852691531181335\n",
      "acc_val: 0.7475\n",
      "Epoch 180, training loss: 0.05091455578804016\n",
      "acc_val: 0.7500\n",
      "Epoch 190, training loss: 0.064822718501091\n",
      "acc_val: 0.7450\n",
      "=== picking the best model according to the performance on validation ===\n"
     ]
    }
   ],
   "source": [
    "#%%\n",
    "from models.GCN import GCN\n",
    "from models.GAT import GAT\n",
    "from models.GIN import GIN\n",
    "test_model = model_construct(args,args.test_model,data).to(device) \n",
    "if(args.test_model == 'GraphSage' or args.test_model == 'GAT'):\n",
    "    poison_adj = to_dense_adj(poison_edge_index, edge_attr=poison_edge_weights)\n",
    "    poison_edge_index, poison_edge_weights = dense_to_sparse(poison_adj)\n",
    "test_model.fit(poison_x, poison_edge_index, poison_edge_weights, poison_labels, bkd_tn_nodes, idx_val,train_iters=200,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target class rate on Vs: 0.8824\n",
      "accuracy on clean test nodes: 0.8050\n",
      "ASR: 0.7700\n",
      "CA: 0.7550\n"
     ]
    }
   ],
   "source": [
    "output = test_model(poison_x,poison_edge_index,poison_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[idx_attach]==args.target_class).float().mean()\n",
    "print(\"target class rate on Vs: {:.4f}\".format(train_attach_rate))\n",
    "#%%\n",
    "induct_edge_index = torch.cat([poison_edge_index,mask_edge_index],dim=1)\n",
    "induct_edge_weights = torch.cat([poison_edge_weights,torch.ones([mask_edge_index.shape[1]],dtype=torch.float,device=device)])\n",
    "# idx_test = data.test_mask.nonzero().flatten()[:200]\n",
    "# idx_test = list(set(data.test_mask.nonzero().flatten().tolist()) - set(atk_test_nodes))\n",
    "# idx_atk = data.test_mask.nonzero().flatten()[200:].tolist()\n",
    "# yt_nids = [nid for nid in idx_atk if data.y.tolist()==args.target_class] \n",
    "# yx_nids = torch.LongTensor(list(set(idx_atk) - set(yt_nids))).to(device)\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[atk_test_nodes] = args.target_class\n",
    "clean_acc = test_model.test(poison_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "'''clean accuracy of clean test nodes before injecting triggers to the attack test nodes'''\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "'''inject trigger on attack test nodes (idx_atk)'''\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(atk_test_nodes,poison_x,induct_edge_index,induct_edge_weights)\n",
    "'''do pruning in test datas'''\n",
    "if(args.defense_mode == 'prune' or args.defense_mode == 'isolate'):\n",
    "    induct_edge_index,induct_edge_weights = prune_unrelated_edge(args,induct_edge_index,induct_edge_weights,induct_x,device)\n",
    "'''attack evaluation'''\n",
    "asr = test_model.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,atk_test_nodes)\n",
    "ca = test_model.test(induct_x,induct_edge_index,induct_edge_weights,data.y,clean_test_nodes)\n",
    "print(\"ASR: {:.4f}\".format(asr))\n",
    "print(\"CA: {:.4f}\".format(ca))\n",
    "# output = test_model(induct_x,induct_edge_index,induct_edge_weights)\n",
    "# train_attach_rate = (output.argmax(dim=1)[atk_test_nodes]==args.target_class).float().mean()\n",
    "# print(\"ASR: {:.4f}\".format(train_attach_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "from torch_geometric.utils import to_dense_adj,dense_to_sparse\n",
    "sp_induct_x = help_funcs.normalize(sp.csr_matrix(induct_x.cpu().detach().numpy()))\n",
    "sp_induct_adj = help_funcs.normalize_adj(sp.csr_matrix(to_dense_adj(induct_edge_index)[0].cpu().detach().numpy()))\n",
    "induct_x = torch.FloatTensor(np.array(sp_induct_x.todense())).to(device)\n",
    "induct_adj = torch.FloatTensor(np.array(sp_induct_adj.todense())).to(device)\n",
    "induct_edge_index,induct_edge_weights = dense_to_sparse(induct_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "train_attach_rate = (output.argmax(dim=1)[yx_nids]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(train_attach_rate))\n",
    "clean_acc = gcn.test(induct_x,induct_edge_index,induct_edge_weights,data.y,idx_test)\n",
    "asr = gcn.test(induct_x,induct_edge_index,induct_edge_weights,atk_labels,idx_atk)\n",
    "print(\"accuracy on clean test nodes: {:.4f}\".format(clean_acc))\n",
    "print(\"ASR1: {:.4f}\".format(asr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_gcn = GCN(nfeat=data.x.shape[1],\\\n",
    "            nhid=args.hidden,\\\n",
    "            nclass= int(data.y.max()+1),\\\n",
    "            dropout=args.dropout,\\\n",
    "            lr=args.lr,\\\n",
    "            weight_decay=args.weight_decay,\\\n",
    "            device=device).to(device)\n",
    "#%%\n",
    "atk_labels = poison_labels.clone()\n",
    "atk_labels[idx_atk] = args.target_class\n",
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "benign_gcn.fit(data.x, data.edge_index, edge_weights, data.y, idx_train, idx_val,train_iters=args.epochs,verbose=True)\n",
    "benign_output = benign_gcn(data.x, data.edge_index, edge_weights)\n",
    "benign4poison_output = benign_gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "benign_ca = (benign_output.argmax(dim=1)[idx_test]==data.y[idx_test]).float().mean()\n",
    "benign4poison_ca = (benign4poison_output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"BenignCA: {:.4f}\".format(benign_ca))\n",
    "print(\"Benign for poisoning CA: {:.4f}\".format(benign4poison_ca))\n",
    "print((benign_output.argmax(dim=1)[yx_nids]==args.target_class).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atk_labels = data.y.clone()\n",
    "idx_atk = obtain_attach_nodes(data.test_mask.nonzero().flatten(), 200)\n",
    "can_test_nodes = torch.LongTensor(list(set(data.test_mask.nonzero().flatten()) - set(idx_atk))).to(device)\n",
    "idx_test = obtain_attach_nodes(can_test_nodes,200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weights = torch.ones([data.edge_index.shape[1]],device=device,dtype=torch.float)\n",
    "induct_x, induct_edge_index,induct_edge_weights = model.inject_trigger(idx_atk,data.x,data.edge_index,edge_weights)\n",
    "output = gcn(induct_x,induct_edge_index,induct_edge_weights)\n",
    "test_asr= (output.argmax(dim=1)[idx_atk]==args.target_class).float().mean()\n",
    "print(\"ASR: {:.4f}\".format(test_asr))\n",
    "test_ca = (output.argmax(dim=1)[idx_test]==atk_labels[idx_test]).float().mean()\n",
    "print(\"CA: {:.4f}\".format(test_ca))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2143, device='cuda:2') 2723\n",
      "tensor([0.1086], device='cuda:2')\n",
      "tensor(2081, device='cuda:2') 2729\n",
      "tensor([0.1186], device='cuda:2')\n",
      "tensor(1780, device='cuda:2') 2732\n",
      "tensor([0.1482], device='cuda:2')\n",
      "tensor(1791, device='cuda:2') 2735\n",
      "tensor([0.1447], device='cuda:2')\n",
      "tensor(1775, device='cuda:2') 2738\n",
      "tensor([0.1165], device='cuda:2')\n",
      "tensor(133, device='cuda:2') 2741\n",
      "tensor([0.1004], device='cuda:2')\n",
      "tensor(1699, device='cuda:2') 2744\n",
      "tensor([0.1272], device='cuda:2')\n",
      "tensor(1251, device='cuda:2') 2747\n",
      "tensor([0.1110], device='cuda:2')\n",
      "tensor(843, device='cuda:2') 2750\n",
      "tensor([0.1307], device='cuda:2')\n",
      "tensor(1782, device='cuda:2') 2753\n",
      "tensor([0.1010], device='cuda:2')\n",
      "tensor(303, device='cuda:2') 2756\n",
      "tensor([0.1448], device='cuda:2')\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "poison_adj_dense = to_dense_adj(poison_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = list(range(data.x.shape[0],poison_x.shape[0]))\n",
    "for nid in idx_attach:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = poison_adj_dense[0][nid].nonzero()\n",
    "    # bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(poison_x[u].unsqueeze(0),poison_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2535 2759\n",
      "tensor([0.1087], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "449 2816\n",
      "tensor([0.1013], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "2083 2858\n",
      "tensor([0.1088], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "2083 3251\n",
      "tensor([0.1088], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1731 2867\n",
      "tensor([0.1448], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1731 3323\n",
      "tensor([0.1448], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1730 2879\n",
      "tensor([0.1103], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "2085 2921\n",
      "tensor([0.1213], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "139 2933\n",
      "tensor([0.1166], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "139 2969\n",
      "tensor([0.1166], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "139 2933\n",
      "tensor([0.1166], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "139 2969\n",
      "tensor([0.1166], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "338 2978\n",
      "tensor([0.1012], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1638 3008\n",
      "tensor([0.1286], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1638 3065\n",
      "tensor([0.1286], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1390 3041\n",
      "tensor([0.1011], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1257 3056\n",
      "tensor([0.1138], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1638 3008\n",
      "tensor([0.1286], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1638 3065\n",
      "tensor([0.1286], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "234 3125\n",
      "tensor([0.1415], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1499 3167\n",
      "tensor([0.1110], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "660 3179\n",
      "tensor([0.1065], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1589 3194\n",
      "tensor([0.1087], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "2083 2858\n",
      "tensor([0.1088], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "2083 3251\n",
      "tensor([0.1088], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1339 3266\n",
      "tensor([0.1010], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "615 3272\n",
      "tensor([0.1060], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "672 3302\n",
      "tensor([0.1258], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "251 3308\n",
      "tensor([0.1066], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1731 2867\n",
      "tensor([0.1448], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1731 3323\n",
      "tensor([0.1448], device='cuda:2', grad_fn=<SumBackward1>)\n",
      "1805 3341\n",
      "tensor([0.1034], device='cuda:2', grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "induct_adj_dense = to_dense_adj(induct_edge_index)\n",
    "def edge_sim_analysis(edge_index, features):\n",
    "    sims = []\n",
    "    for (u,v) in edge_index:\n",
    "        sims.append(float(F.cosine_similarity(features[u].unsqueeze(0),features[v].unsqueeze(0))))\n",
    "    sims = np.array(sims)\n",
    "    # print(f\"mean: {sims.mean()}, <0.1: {sum(sims<0.1)}/{sims.shape[0]}\")\n",
    "    return sims\n",
    "\n",
    "bkd_nids = induct_x.shape[0] - poison_x.shape[0]\n",
    "for nid in atk_test_nodes:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        if(v in bkd_nids):\n",
    "            u = nid\n",
    "            print(nid,v)\n",
    "            print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_nids = list(range(poison_x.shape[0],induct_x.shape[0]))\n",
    "for nid in idx_test:\n",
    "    # polished_dr_test = copy.deepcopy(bkd_dr_test)\n",
    "    # polished_adj_nodes = polished_dr_test.data['mat_adj'].to_dense()[nid].nonzero()\n",
    "    polished_adj_nodes = induct_adj_dense[0][nid].nonzero()\n",
    "    for v in polished_adj_nodes:\n",
    "        v = int(v)\n",
    "        # if(v in bkd_nids):\n",
    "        u = nid\n",
    "        print(nid,v)\n",
    "        print(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_graph_homophily(adj,x,device):\n",
    "    deg_vector = adj.sum(1)\n",
    "    deg_matrix = torch.diag(adj.sum(1)).to(device)\n",
    "    deg_matrix += torch.eye(len(adj)).to(device)\n",
    "    deg_inv_sqrt = deg_matrix.pow(-0.5)\n",
    "    deg_inv_sqrt.masked_fill_(deg_inv_sqrt == float('inf'), 0.)\n",
    "    adj = torch.matmul(deg_inv_sqrt,adj)\n",
    "    adj = torch.matmul(adj,deg_inv_sqrt)\n",
    "    x_neg = adj @ x\n",
    "    node_sims = np.array([float(F.cosine_similarity(xn.unsqueeze(0),xx.unsqueeze(0))) for (xn,xx) in zip(x_neg,x)])   \n",
    "    # node_sims = np.array([torch.round(i,decimals=2) for i in node_sims])\n",
    "    # print(node_sims)\n",
    "    return node_sims\n",
    "bkd_graph_test_node_sims = calculate_graph_homophily(to_dense_adj(data.edge_index)[0].to(device),data.x.to(device),device)\n",
    "bkd_graph_train_node_sims = calculate_graph_homophily(to_dense_adj(poison_edge_index)[0].to(device),poison_x.to(device),device)\n",
    "clean_graph_node_sims = calculate_graph_homophily(to_dense_adj(induct_edge_index)[0].to(device),induct_x.to(device),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "def to_percent(y,position):\n",
    "    return str(100*y)+\"%\"#round\n",
    "\n",
    "plt.hist(clean_graph_node_sims,bins=10,weights=[1./len(clean_graph_node_sims)]*len(clean_graph_node_sims),density=True, alpha=0.75, label='clean')#weights1weightsxseries\n",
    "plt.hist(np.array(bkd_graph_test_node_sims),bins=20,weights=[1./len(bkd_graph_test_node_sims)]*len(bkd_graph_test_node_sims),density=True, alpha=0.75, label='poison')#weights1weightsxseries\n",
    "plt.hist(np.array(bkd_graph_train_node_sims),bins=20,weights=[1./len(bkd_graph_train_node_sims)]*len(bkd_graph_train_node_sims),density=True, alpha=0.75, label='attack')#weights1weightsxseries\n",
    "fomatter=FuncFormatter(to_percent)\n",
    "# plt.gca().yaxis.set_major_formatter(fomatter)\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"Cosine Similarity\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.png\")\n",
    "# plt.savefig(\"pics/grb_cora_node_sims.pdf\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bkd_test_edge_index = to_dense_adj(data.edge_index)[0].nonzero()\n",
    "trigger_trigger_edge_sims = []\n",
    "trigger_trigger_edge_index = []\n",
    "\n",
    "trigger_target_edge_sims = []\n",
    "trigger_target_edge_index = []\n",
    "\n",
    "normal_normal_edge_sims = []\n",
    "normal_normal_edge_index = []\n",
    "\n",
    "trigger_normal_edge_sims = []\n",
    "trigger_normal_edge_index = []\n",
    "\n",
    "target_target_edge_sims = []\n",
    "target_target_edge_index = []\n",
    "for (u,v) in bkd_test_edge_index:\n",
    "    if ((v,u) in trigger_trigger_edge_index) or ((u,v) in trigger_trigger_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_target_edge_index) or ((u,v) in trigger_target_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in normal_normal_edge_index) or ((u,v) in normal_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in trigger_normal_edge_index) or ((u,v) in trigger_normal_edge_index):\n",
    "        continue\n",
    "    if ((v,u) in target_target_edge_index) or ((u,v) in target_target_edge_index):\n",
    "        continue\n",
    "    \n",
    "    if (u in bkd_nids) and (v in bkd_nids):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_trigger_edge_sims.append(edge_sims)\n",
    "        trigger_trigger_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_atk)) or ((v in bkd_nids) and (u in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_target_edge_sims.append(edge_sims)\n",
    "        trigger_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    if (u in idx_test) and (v in idx_test):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        normal_normal_edge_sims.append(edge_sims)\n",
    "        normal_normal_edge_index.append((u,v))\n",
    "        continue\n",
    "    if ((u in bkd_nids) and (v in idx_test)) or ((v in bkd_nids) and (u in idx_test)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        trigger_normal_edge_sims.append(edge_sims)\n",
    "        trigger_normal_edge_index.append((u,v))\n",
    "\n",
    "    if ((u in idx_atk) and (v in idx_atk)):\n",
    "        edge_sims = float(F.cosine_similarity(induct_x[u].unsqueeze(0),induct_x[v].unsqueeze(0)))\n",
    "        target_target_edge_sims.append(edge_sims)\n",
    "        target_target_edge_index.append((u,v))\n",
    "        continue\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('py38_torch110')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c4260eba67904b42d68a3963bc583366103d86fb6c89846e20de6072b78e7707"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
